# -*- coding: utf-8 -*-
"""Copy of LOAN APPROVAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZRqmNw0oMUUHzXJSsGqWURTV3cy8VR2H
"""

import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.metrics import accuracy_score
import xgboost as xgb
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import f_oneway # ANOVA Analysis
from scipy.stats import ttest_ind # Conducting a T test
from scipy.stats import chi2_contingency # Statistical model for performing a chi squared analysis
data = pd.read_csv("loan_approval_dataset.csv")

data.head()

data.shape

data.info()

data.describe()

data.duplicated(keep=False).sum()

data[' residential_assets_value'] = data[' residential_assets_value'].abs()
(data[' residential_assets_value'] < 0).sum()

# Removing whitespaces from column names
data.columns = data.columns.str.strip()

# Removing white spaces from values in the dataframe
data = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)

data.info()

# Adding CIBIL Rating column with values (Poor, Average, Good and Excellent)

def cibil_rating(value):
    if 300 <= value <= 549:
        return "Poor"
    elif 550 <= value <= 649:
        return "Average"
    elif 650 <= value <= 749:
        return "Good"
    elif 750 <= value <= 900:
        return "Excellent"
    else:
        return "Error"

data['CIBIL_rating'] = data['cibil_score'].apply(cibil_rating)

def income_level(value):
    if 100000 <= value <= 1000000:
        return "Low"
    elif 1000001 <= value <= 4000000:
        return "Low-Middle"
    elif 4000001 <= value <= 7500000:
        return "Upper-Middle"
    elif 7500001 <= value <= 10000000:
        return "High"
    else:
        return "Very High Income"

data['Income level'] = data['income_annum'].apply(income_level)

def loan_rating(value):
    if 100000 <= value <= 10000000:
        return "Bronze"
    elif 10000001 <= value <= 20000000:
        return "Silver"
    elif 20000001 <= value <= 30000000:
        return "Gold"
    elif 30000001 <= value <= 40000000:
        return "Platinum"
    else:
        return "Unknown"

data['Loan Rating'] = data['loan_amount'].apply(loan_rating)

def loan_term(value):
    if 0 <= value <= 7:
        return "Short Term"
    elif 8 <= value <= 14:
        return "Intermediate"
    elif 15 <= value <= 20:
        return "Long Term"
    else:
        return "Unknown"

data['Loan Term Type'] = data['loan_term'].apply(loan_term)

def dependents(value):
    if 0 <= value <= 1:
        return "Low"
    elif 2 <= value <= 3:
        return "Moderate"
    elif 4 <= value <= 5:
        return "High"
    else:
        return "Unknown"

data['Dependent Level'] = data['no_of_dependents'].apply(dependents)

data.head()

# Group data based on income level

income = data['Income level'].value_counts().reset_index()
income = income.rename(columns={'Income level':'Income Levels', 'count': "Number of Applicants"})
income

# Group based on Number of dependents

dependents = data['Dependent Level'].value_counts().reset_index()
dependents = dependents.rename(columns={'count': "Number of Applicants"})
dependents

education = data['education'].value_counts().reset_index()
# education = education.rename(columns={'education':'Education Levels', 'count': "Number_of_Applicants"})
education

employment = data['self_employed'].value_counts().reset_index()
employment = employment.rename(columns={'self_employed':'Self Employed', 'count': "Number_of_Applicants"})
employment

loans = data['loan_status'].value_counts().reset_index()
loans = loans.rename(columns={'loan_status':'Loan Status', 'count': "Total"})
loans

# Grouping data based on Level of Education and Loan approval Status
education = data.groupby(['education', 'loan_status']).size().reset_index()

# Filter to include Approved and Rejected
education = education[education['loan_status'].isin(['Approved', 'Rejected'])]

education

# Pivot Table
pivot_table = education.pivot(index='education', columns='loan_status', values=0)
rows_total = pivot_table.sum(axis=1)
percentage = pivot_table.div(rows_total, axis=0) * 100

# Creating heatmap
plt.figure(figsize=(6,4))
sns.heatmap(percentage, annot=True, fmt='.1f', cmap='coolwarm', linewidth=0.5)

# Creating Labels
plt.title("Loan Approval by Education Level")
plt.xlabel("Loan Status")
plt.ylabel("Education Level")

plt.show()

# Grouping data based on the Type of Employment and Loan approval Status
employment = data.groupby(['self_employed', 'loan_status']).size().reset_index()

# Filter to include Approved and Rejected
employment = employment[employment['loan_status'].isin(['Approved', 'Rejected'])]
employment = employment.rename(columns={0:'Total'})

employment

# Pivot Table
pivot_table = employment.pivot(index='self_employed', columns='loan_status', values='Total')
rows_total = pivot_table.sum(axis=1)
percentage = pivot_table.div(rows_total, axis=0) * 100

# Creating heatmap
plt.figure(figsize=(6,4))
sns.heatmap(percentage, annot=True, fmt='.1f', cmap='coolwarm', linewidth=0.5)

# Creating Labels
plt.title("Loan Approval by Type of Employment")
plt.xlabel("Loan Status")
plt.ylabel("Self Employed")

plt.show()

# Grouping data based on the CIBIL Rating and Loan approval Status
cibil_rating = data.groupby(['CIBIL_rating', 'loan_status']).size().reset_index()
cibil_rating = cibil_rating.rename(columns={0:'Total'})

cibil_rating

obj = (data.dtypes == 'object')
print("Categorical variables:",len(list(obj[obj].index)))

# Pivot Table
pivot_table = cibil_rating.pivot(index='CIBIL_rating', columns='loan_status', values='Total')
pivot_table['Approval Rate'] = pivot_table['Approved'] / (pivot_table['Approved'] + pivot_table['Rejected']) * 100

# Creating heatmap
plt.figure(figsize=(8,5))
sns.heatmap(pivot_table[['Approval Rate']], annot=True, fmt='.1f', cmap='coolwarm', linewidth=0.5)

# Creating Labels
plt.title("Loan Approval Rate by CIBIL Rating")
plt.xlabel("Loan Status")
plt.ylabel("CIBIL Rating")

plt.show()

correlation_matrix = data.corr(numeric_only=True)

# Creating heatmap
plt.figure(figsize=(7, 4))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', center=0, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Correlation between asset value and income
assets = ['residential_assets_value', 'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']
income = 'income_annum'

# Calculate the correlation(Pearson Correlation)
correlations = data [assets + [income]].corr()

correlations.head()

# Correlation between asset value and Loan
assets = ['residential_assets_value', 'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']
loan = 'loan_amount'

# Calculate the correlation(Pearson Correlation)
correlations = data [assets + [loan]].corr()

correlations.head()

# Splitting data into groups
approved = data[data['loan_status'] == "Approved"]
rejected = data[data['loan_status'] == "Rejected"]

# Perform a T-test
assets = ['residential_assets_value', 'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']

for column in assets:
    t_stat, p_value = ttest_ind(approved[column], rejected[column], equal_var=False)
    print(f"T-Test for {column}: ")
    print(f"  T-statistic: {t_stat}")
    print(f"  P-Value: {p_value}")
    if p_value < 0.05: # 95% Confidence Interval
        print(f"The difference in {column} between approved and rejected loans is statistically significant.")
    else:
        print(f"The difference in {column} between approved and rejected loans is not statistically significant.")

# Null Hypothesis: There is no relation(Significant influence) between assets and income.
# Alternate Hypothesis: There is a relation(Significant Influence) between assets and income.

# Extracting values in the columns
residential_asset = data['residential_assets_value']
commercial_asset = data['commercial_assets_value']
luxury_asset = data['luxury_assets_value']
bank_asset = data['bank_asset_value']
income = data['income_annum']

# Performing ANOVA
f_statistics, p_value = f_oneway(residential_asset, commercial_asset, luxury_asset, bank_asset, income)

print(f"The F-Statistics: {f_statistics}")
print(f"P-Value: {p_value}")

# interpretation of results
if p_value < 0.05:
    print("There is a significant influence of income on assets.")
else:
    print("There is no signinficant influence of income on assets.")

# Splitting data into groups
income = data['income_annum']
loan = data['loan_amount']


t_stat, p_value = ttest_ind(income, loan, equal_var=False)

print(f"  T-statistic: {t_stat}")
print(f"  P-Value: {p_value}")
if p_value < 0.05: # 95% Confidence Interval
    print(f"The difference in {column} between approved and rejected loans is statistically significant.")
else:
    print(f"The difference in {column} between approved and rejected loans is not statistically significant.")

# Splitting data into groups
income = data['income_annum']
loan = data['loan_amount']

plt.figure(figsize=(6,4))
ax = plt.axes()

ax.boxplot([income, loan], labels=["Income Per Annum", "Loan Amount"])

plt.xlabel("Income vs Loan")
plt.ylabel("Amount")
plt.title("Box Plot of Income and Loan Amount")
plt.show()

data.columns

# Creating Contingency table
contingency_table = pd.crosstab(data['loan_status'], data['Dependent Level'])

# perform Chi squared test
chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)

print(f"Chi Squared Test Statistics: {chi2}")
print(f"P-Value Statistics: {p_value}")
print(f"DOF: {dof}")

if p_value < 0.05:
    print("There is a statistically significant relation between loan status and number of dependents.")
else:
    print("There is no statistically significant relation between loan status and number of dependents.")

def edu_status(status):
    if status == "Graduate":
        return 1
    elif status == "Not Graduate":
        return 0
    else:
        return "Unknown"
data['edu_status'] = data['education'].apply(edu_status)

def type_employment(employment):
    if employment == "Yes":
        return 1
    elif employment == "No":
        return 0
    else:
        return "Unknown"
data['type_employment'] = data['self_employed'].apply(type_employment)

def loan_approval(status):
    if status == "Approved":
        return 1
    elif status == "Rejected":
        return 0
    else:
        return "Unknown"
data['loan_approval'] = data['loan_status'].apply(loan_approval)

data.head()

# Seperating the x-axis values

X = data.drop(columns=['loan_id', 'education', 'CIBIL_rating', "Income level",
                     "Loan Rating", 'Loan Term Type', 'Dependent Level', 'loan_status', "loan_approval", 'self_employed'])

X

# Define the y-axis
y = data['loan_approval']

y.head()

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Splitting the data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#KNN CLASSIFIER
#RANDOM FOREST
#SVC
#LOGISTIC REGRESSION
#DECISION TREE
#NAIVE BYERS
#XGB
#GBM

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier


from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import StackingClassifier

# Initialize base classifiers
base_classifiers = [
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
    ('dt', DecisionTreeClassifier(max_depth=5, random_state=42))
]

# Initialize stacking classifier with a meta-classifier (Logistic Regression in this case)
stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=LogisticRegression())

# Initialize classifiers
knn = KNeighborsClassifier(n_neighbors=3)
rfc = RandomForestClassifier(n_estimators=7, criterion='entropy', random_state=7)
svc = SVC()
lc = LogisticRegression()
dtc = DecisionTreeClassifier()
nb = GaussianNB()

# Initialize XGBoost classifier
xgb_clf = xgb.XGBClassifier(
    objective='binary:logistic',  # for binary classification
    eval_metric='error'            # evaluation metric
)
# Initialize GBM classifier
gbm_clf = GradientBoostingClassifier(
    n_estimators=100,  # number of boosting stages
    learning_rate=0.1, # learning rate
    max_depth=3        # maximum depth of the individual estimators
)
# Initialize AdaBoost classifier
ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)

# Initialize MLPClassifier (Neural Network)
nn_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, alpha=0.0001, solver='adam', random_state=42)

# Initialize base classifier (Decision Tree in this case)
base_classifier = DecisionTreeClassifier()

# Initialize Bagging Classifier
bagging_clf = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)

# Train and print accuracy for each classifier
for clf in (rfc, knn, svc, lc, dtc, nb, xgb_clf, gbm_clf,ada_clf,nn_clf,bagging_clf,stacking_clf):
    clf.fit(X_train, y_train)
    Y_pred = clf.predict(X_train)
    print("Accuracy score of", clf.__class__.__name__, "=", 100*metrics.accuracy_score(Y_train, Y_pred))

from sklearn.metrics import classification_report

# Making predictions on the testing set
for clf in (rfc, knn, svc, lc, dtc, nb, xgb_clf, gbm_clf,ada_clf,nn_clf,bagging_clf,stacking_clf):
    clf.fit(X_train, Y_train)
    Y_pred = clf.predict(X_test)
    print("Accuracy score of", clf.__class__.__name__, "=", 100 * accuracy_score(Y_test, Y_pred))
    # Classification report
    print("Classification Report:")
    print(classification_report(Y_test, Y_pred))